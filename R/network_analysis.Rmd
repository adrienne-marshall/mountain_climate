---
title: "Mountain Climate Network Analysis"
author: "mountain climate research group"
date: "7/12/2017"
output:
  html_document:
    toc: true
    toc_depth: 2
    toc_float: true
---

#Data preparation
Load packages
```{r setup, message = FALSE, warning = FALSE}
library(tidyverse)
library(tidytext)
library(igraph)
library(ggraph)
library(stringr)
library(widyr)
library(knitr)
library(topicmodels)
library(ggthemes)
```

Get data.
```{r read data, cache = TRUE, message = FALSE}
dat <- read_csv("data/final_corpus.csv") %>%
  filter(!is.na(title))
```

Make a data frame that lists all the authors in separate columns for each paper.
```{r list authors, ache = TRUE}
author_df <- dat %>% select(id, authors) %>% 
  unnest_tokens(output = authors_long, input = authors, token = stringr::str_split, pattern = ";")
```

Convert each author name to "last, first initial" format.
```{r author formatting, cache = TRUE}
author_df <- author_df %>% 
  mutate(authors_long = str_trim(authors_long)) %>%
  mutate(last = ifelse(grepl(",", authors_long) == TRUE,
    str_extract(authors_long, "[^,]*"),
    str_extract(authors_long, "[^ ]*$"))) %>%
  mutate(first_init = ifelse(grepl(",", authors_long) == TRUE,
                             strsplit(authors_long, " "),
                             str_sub(authors_long, start = 1, end = 1))) %>%
  unnest(first_init) %>%
  group_by(authors_long, id) %>%
  slice(2) %>%
  ungroup() %>%
  arrange(id) %>%
  mutate(first_init = str_sub(first_init, 1, 1)) %>%
  mutate(author = paste0(first_init, ". ", last)) %>%
  select(-last, -first_init)

kable(head(author_df))
write_csv(author_df, "data/authors.csv")

```


Now column "author" contains the most standard version.
It looks like we have `r length(unique(author_df$author))` unique authors. 

Get pairwise author count:
```{r pairwise authors, cache = TRUE}
author_pairs <- author_df %>%
  pairwise_count(author, id, sort = TRUE, upper = FALSE)
names(author_pairs)[1:2] <- c("author1", "author2")
kable(head(author_pairs))

```

```{r authors_matrix, cache = TRUE, eval = FALSE, echo = FALSE}
#Make a matrix of authors for use in gephi. 
author_mat <- spread(author_pairs, key = author2, value = n)
author_mat[is.na(author_mat)] <- 0
names(author_mat)[1] <- ""
write_csv(author_mat, "data/author_matrix.csv")
```


Make a yearly version of author pairs as well: 
```{r pairwise authors year, cache = TRUE}
author_df <- full_join(author_df, dat[, c("id", "year")], by = "id")
author_df <- author_df %>% filter(!is.na(year))

years <- unique(author_df$year)
pair_dfs <- vector("list", length(years))
for(i in 1:length(years)){
  df <- author_df %>% filter(year == years[i])
  pair_dfs[[i]] <- df %>% 
    pairwise_count(author, id, sort = TRUE, upper = FALSE) %>%
    mutate(year = years[i])
}

author_pairs_years <- bind_rows(pair_dfs)
names(author_pairs_years)[1:2] <- c("author1", "author2")

#Subset to years with > 50 author collaborations in our dataset.
#(save a copy with the full dataset for later).
apy_full <- author_pairs_years
years <- author_pairs_years %>% group_by(year) %>% count() %>% filter(nn > 50)
author_pairs_years <- author_pairs_years %>% filter(year %in% years$year)

```

# Network analysis:

## Simple network of authors.
Plot network of author collaborations in cases where there are 2 or more collaborations.

```{r collab_network, fig.width = 10, cache = TRUE}
set.seed(1234)
author_pairs%>%
  filter(n > 2) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "dh") +
  geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = "cyan4") +
  geom_node_point(size = 2) +
  geom_node_text(aes(label = name), repel = TRUE, 
                 point.padding = unit(0.2, "lines"),
                 size = 2) +
  theme_void()
```

Looks like this tells us that most of the researchers in this network are operating as isolated small groups, rather than having strong inter-group collaboration - although, collaborations with less than 3 counts were left out in the interest of computational efficiency, so that may be hiding some interesting collaboration structure. Go ahead and make the full network (don't include text; there will be 1897 authors - which also means that there are 69 authors who have only sole-authored items). This takes ~5 minutes.

```{r full author network, cache = TRUE, fig.width = 10}
set.seed(1234)
author_pairs%>%
  graph_from_data_frame() %>%
  ggraph(layout = "kk") +
  geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = "cyan4") +
  geom_node_point(size = 1) +
  geom_node_text(aes(label = name), repel = FALSE, 
                 point.padding = unit(0.2, "lines"),
                 size = 2) +
  theme_void()

```

Wow, that makes it look like there are a bunch of really central authors that connect to more peripheral ones. How do other network layouts affect this?

```{r full author network2, cache = TRUE, fig.width = 10}
set.seed(1234)
author_pairs%>%
  graph_from_data_frame() %>%
  ggraph(layout = "drl") +
  geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = "cyan4") +
  geom_node_point(size = 0.1) +
  #geom_node_text(aes(label = name), repel = FALSE, 
  #               point.padding = unit(0.2, "lines"),
  #               size = 2) +
  theme_void()

```


How about a hive plot? This would solve the problem that different layouts suggest different information about the network. There are probably a number of ways we could structure a hive plot - but one would be to have three axes, representing physical sciences, social sciences, and life sciences - and then each axis would be doubled, so we could see collaborations within the axis, and collaborations across disciplines. 
```{r hive plot, cache = TRUE, eval = TRUE, fig.width = 9}
graph <- graph_from_data_frame(author_pairs, directed = FALSE)
V(graph)$friends <- degree(graph, mode = 'all')
V(graph)$friends <- ifelse(V(graph)$friends < 3, 'few', 
                           ifelse(V(graph)$friends >= 10, 'many', 'medium'))
ggraph(graph, 'hive', axis = 'friends') + 
    geom_edge_hive(aes(edge_alpha = n)) + 
    geom_axis_hive(aes(colour = friends), 
                   alpha = 1, size = 2, label = FALSE) + 
    coord_fixed() +
    labs(color = "Collaborators") +
    theme_void()
```


Now, try a graph that facets the networks by year. First, do this wtih only author pairs with greater than 1 collaboration - this may not be the most interesting piece of information, but it's a little easier to read than a version with all the authors included.
```{r author_network_year sparse, cache = TRUE, fig.width = 10, echo = FALSE}
set.seed(1234)
author_pairs_years%>%
  filter(n > 1) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "kk") +
  geom_edge_link(aes(edge_width = n), edge_colour = "cyan4", edge_alpha = 0.7) +
  geom_node_point(size = 0.2) +
  #geom_node_text(aes(label = name), repel = FALSE, 
   #              point.padding = unit(0.2, "lines"),
    #             size = 2) +
  facet_wrap(~year) + 
  theme_void()
```
It does look like we might be moving towards more networks, and possibly more interconnectedness of networks over time? There are really dense clumps in 2008-2010, and a greater number of clumps with more interaction between them in more recent years? However, this interpretation depends strongly on what graphical layout we use; this would be a good thing to research more.  

How about if we do this with all collaborations included?

```{r author_network_year dense, cache = TRUE, fig.width = 10, echo = FALSE}
set.seed(1234)
author_pairs_years%>%
  graph_from_data_frame() %>%
  ggraph(layout = "lgl") +
  geom_edge_link(aes(edge_width = n), edge_colour = "cyan4", edge_alpha = 0.7) +
  geom_node_point(size = 0.2) +
  #geom_node_text(aes(label = name), repel = FALSE, 
   #              point.padding = unit(0.2, "lines"),
    #             size = 2) +
  facet_wrap(~year) + 
  theme_void()
```

### Descriptive statistics
#### Actor-level statistics
Finally, let's calculate some descriptive statistics on actors in this network - this is just a start, and a place we'll need to come back to. First, eigenvector centrality:
```{r eigen_statistics, cache = TRUE}
graph_obj <- graph_from_data_frame(author_pairs, directed = FALSE)
ec <- eigen_centrality(graph_obj)[[1]]
ec_df <- data.frame(ec) 
ec_df <- ec_df %>% mutate(name = rownames(ec_df)) %>%
  arrange(desc(ec))
names(ec_df[1]) <- "eigen_centrality"

kable(ec_df[1:20,])
```

Wow, the dominance of UI people in this list is actually sort of disturbing...? Eigenvector centrality can be thought of as "the sum of an actor's connections to other actors, weighted by those other actors' degree centrality" (Bodin and Prell 2011, originally Borgatti, 1995).

Let's calculate some other metrics so we have a good summary.

```{r actor statistics, cache = TRUE}
bet <- betweenness(graph_obj) %>%
  data.frame() 
bet <- bet %>% mutate(name = rownames(bet))
names(bet)[1] <- "betweenness"

deg <- degree(graph_obj) %>% 
  data.frame()
deg <- deg %>% mutate(name = rownames(deg))
names(deg)[1] <- "degree_centrality"

actor_stats <- full_join(ec_df, bet, by = "name") %>%
  full_join(deg, by = "name") %>%
  select(name, everything()) %>%
  arrange(desc(degree_centrality)) 

kable(actor_stats[1:20,])

```

The order of actors changes substantially depending on what metric is used, which is kind of interesting to note (although frankly, J Abatzoglou is pretty much killing it no matter which metric we use). This analysis becomes even more interesting (in my opinion) if we can pair it with more information about these authors: their disciplines, genders, and institutions. 

#### Network level statistics
Calculating some network-level statistics will let us summarize network characteristics and look at changes over time.
```{r network_stats, cache = TRUE}
centr <- centr_degree(graph_obj)
centr$centralization
centr$theoretical_max
```

Wow, that is very low centralization relative to the theoretical maximum. How does centralization change over time? 
```{r network stats over time, cache = TRUE}
years <- unique(apy_full$year)

centr_time <- data.frame(year = years, centralization = NA, tmax = NA)
for(i in 1:length(years)){
  df <- apy_full %>% filter(year == years[i])
  graph_obj <- graph_from_data_frame(df)
  x <- centr_degree(graph_obj)
  centr_time$centralization[i] <- x$centralization
  centr_time$tmax[i] <- x$theoretical_max
}

#Divide centralization/tmax to see what fraction of max is achieved.
centr_time <- centr_time %>% mutate(f = centralization/tmax)

#Plot
plot_dat <- data.table::melt(centr_time, id.vars = "year")

p <- ggplot(plot_dat, aes(x = year, y = value)) + 
  geom_point() +
  geom_smooth() +
  facet_wrap(~variable, scales = "free", dir = "h")
p

#Plot with log transformation:
p <- p + scale_y_continuous(trans = "log")
p

```

I think the first and third plots are most relevant here: the first is the network centralization overall, and the third ("f") is the fraction of the theoretical maximum centralization that was achieved. When we look at these plots without any kind of transformation, there's not much to see - but when they're transformed on a log scale, it looks like there might be an interesting pattern in degree centralization. One caution though is not to over-interpret this until we absolutely have our final corpus. 

Another idea for descriptive statistics is to calculate some subgroup statistics??

# Topic modeling
Join authors with subject data (using journal, discipline, keyword?). A possible approach here is to do some topic modeling with the keywords (276 papers are missing keyword data), categorize papers by topic, and then do network analysis grouped by topic. This is probably a good idea anyway because we want to split papers by topic for when we code for content.

```{r keyword tidy, cache = TRUE}
keyword_df <- dat %>%
  dplyr::select(id, keywords) %>%
  mutate(keywords = gsub(",", ";", keywords)) %>%
  unnest_tokens(input = keywords, output = keywords, token = stringr::str_split, pattern = ";") %>%
  mutate(keywords = str_trim(keywords)) %>%
  filter(!is.na(keywords))
```

We've got `r length(unique(keyword_df$keywords))` unique keywords. Let's look at keyword pairs to see how they're grouped. 

```{r keyword pairs, cache = TRUE}
keyword_pairs <- keyword_df %>%
  pairwise_count(keywords, id, sort = TRUE, upper = FALSE)
```

```{r keyword network, cache = TRUE, fig.width = 10}
set.seed(1234)
keyword_pairs %>%
  filter(n >= 10) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = "salmon") +
  geom_node_point(size = 1) +
  geom_node_text(aes(label = name), repel = TRUE,
                 point.padding = unit(0.2, "lines"),
                 size = 3) +
  theme_void()

```

It looks like we have some core concepts, and then some more peripheral concepts. 

```{r get correlations between keywords, cache = TRUE}
keyword_cors <- keyword_df %>% 
  group_by(keywords) %>%
  #filter(n() >= 50) %>%
  pairwise_cor(keywords, id, sort = TRUE, upper = FALSE)

#Take out keyword correlations with correlation 1; these are redundant.
keyword_cors <- keyword_cors %>%
  filter(round(correlation, 3) < 1)

kable(head(keyword_cors))

```

Visualize the network of keyword correlations: 
```{r visualize keyword correlations, cache = TRUE, fig.width = 10}
set.seed(1234)
keyword_cors %>%
  filter(correlation > .705) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "kk") +
  geom_edge_link(aes(edge_alpha = correlation, edge_width = correlation), edge_colour = "darkorchid") +
  geom_node_point(size = 1) +
  geom_node_text(aes(label = name), repel = FALSE,
                 point.padding = unit(0.2, "lines"),
                 size = 3) +
  theme_void()
```

Well, that's extremely difficult to read, but it does look like there's some more meaningful grouping with correlations than there was with just pairwise analysis. 

Let's move on to some topic modeling to see if we can group papers. First, define stop words in addition to the common ones, and get word counts.
```{r stop words and word counts}
my_stop_words <- data_frame(word = c("climate change", "usa"),
                                      lexicon = rep("custom", 2))

word_counts <- keyword_df %>%
  rename(word = keywords) %>%
  anti_join(my_stop_words) %>%
  count(id, word, sort = TRUE) %>%
  ungroup() %>%
  arrange(-n)

word_counts
```

This topic modeling approach may not make a lot of sense since keywords generally only appear once... but continue anyway. 

```{r topic modeling, cache = TRUE, fig.width = 10}
keyword_dtm <- word_counts %>%
  cast_dtm(id, word, n)

keyword_lda <- LDA(keyword_dtm, k = 8, control = list(seed = 1234))
tidy_lda <- tidy(keyword_lda)

top_terms <- tidy_lda %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  group_by(topic, term) %>%    
  arrange(desc(beta)) %>%  
  ungroup() %>%
  mutate(term = factor(paste(term, topic, sep = "__"), 
                       levels = rev(paste(term, topic, sep = "__")))) %>%
  ggplot(aes(term, beta, fill = as.factor(topic))) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  scale_x_discrete(labels = function(x) gsub("__.+$", "", x)) +
  labs(title = "Top 10 terms in each LDA topic",
       x = NULL, y = expression(beta)) +
  facet_wrap(~ topic, ncol = 4, scales = "free")
```

These look like they're somewhat informative, but imperfect. They would probably be more informative if we did this same procedure with full texts or abstracts (which might be a good next step, using crminer).

For now, can we categorize each paper by its topic?
```{r categorize papers, cache = TRUE}
lda_gamma <- tidy(keyword_lda, matrix = "gamma")
id_topic <- lda_gamma %>% 
  group_by(document) %>%
  filter(gamma == max(gamma)) %>%
  ungroup() %>%
  select(document, topic) %>%
  rename(id = document) %>%
  mutate(id = as.integer(id))

Mode <- function(x) {
  ux <- unique(x)
  ux[which.max(tabulate(match(x, ux)))]
}

network_df <- left_join(author_df, id_topic, by = "id") 
author_topics <- network_df %>% 
  group_by(author) %>%
  summarise(topic_new = Mode(topic))
kable(head(author_topics))

#author_pairs <- author_df %>%
#  pairwise_count(author, id, sort = TRUE, upper = FALSE)
#names(author_pairs)[1:2] <- c("author1", "author2")
#kable(head(author_pairs))
```

## Topic modelling with abstracts:

```{r abstract topic model, cache = TRUE}
abstract_df <- dat %>% select(id, abstract) %>%
  filter(!is.na(abstract)) %>%
  unnest_tokens(word, abstract) %>%
  anti_join(stop_words)

abstract_df %>% count(word, sort = TRUE)

#Looks like there are some numbers... maybe get rid of those.
#Also, change everything to lower case. 
abstract_df <- abstract_df %>%
  filter(!grepl("[[:digit:]]", word)) %>%
  mutate(word = tolower(word))

#Need to add a few more stop words: 
abstract_df <- abstract_df %>%
  filter(!word %in% c("org", "http", "xhtml", "xmlns"))
  
library(widyr)
abstract_word_pairs <- abstract_df %>%
  pairwise_count(word, id, sort = TRUE, upper = FALSE)

#this is slow - only run if needed. 
# abstract_cors <- abstract_df %>% 
#   group_by(word) %>%
#   #filter(n() >= 50) %>%
#   pairwise_cor(word, id, sort = TRUE, upper = FALSE)

#Calculate tf_idf.
abstract_tf_idf <- abstract_df %>% 
  count(id, word, sort = TRUE) %>%
  ungroup() %>%
  bind_tf_idf(word, id, n)

```

```{r abstract topic model 2, cache = TRUE}
word_counts <- abstract_df %>%
  count(id, word, sort = TRUE) %>%
  ungroup()
word_counts

abstract_dtm <- word_counts %>%
  cast_dtm(id, word, n)

#This may take a while. 
abstract_lda <- LDA(abstract_dtm, k = 8, control = list(seed = 1234))
tidy_lda <- tidy(abstract_lda)

top_terms <- tidy_lda %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

top_terms

top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  group_by(topic, term) %>%    
  arrange(desc(beta)) %>%  
  ungroup() %>%
  mutate(term = factor(paste(term, topic, sep = "__"), 
                       levels = rev(paste(term, topic, sep = "__")))) %>%
  ggplot(aes(term, beta, fill = as.factor(topic))) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  scale_x_discrete(labels = function(x) gsub("__.+$", "", x)) +
  labs(title = "Top 10 terms in each LDA topic",
       x = NULL, y = expression(beta)) +
  facet_wrap(~ topic, ncol = 4, scales = "free")

```

```{r gamma, cache = TRUE}
lda_gamma <- tidy(abstract_lda, matrix = "gamma")

ggplot(lda_gamma, aes(gamma, fill = as.factor(topic))) +
  geom_histogram(show.legend = FALSE, bins = 40) +
  facet_wrap(~ topic, ncol = 4) +
  scale_y_log10() +
  labs(title = "Distribution of probability for each topic",
       y = "Number of documents", x = expression(gamma))

```

It doesn't look like these are very well separated: in an ideal world where each document fits perfectly into one category, we would just have bars at the far left and right sides of each facet. 

Repeat the topic-modeling with term frequency-inverse document frequency (TF-IDF) instead of word counts: 
```{r tf_idf lda, cache = TRUE, echo = FALSE}
abstract_dtm <- abstract_tf_idf %>% 
  select(id, word, tf_idf) %>%
  mutate(tf_idf_int = round(tf_idf*1000)) %>%
  cast_dtm(id, word, tf_idf_int)

abstract_lda <- LDA(abstract_dtm, k = 8, control = list(seed = 1234))
tidy_lda <- tidy(abstract_lda)

top_terms <- tidy_lda %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

top_terms

top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  group_by(topic, term) %>%    
  arrange(desc(beta)) %>%  
  ungroup() %>%
  mutate(term = factor(paste(term, topic, sep = "__"), 
                       levels = rev(paste(term, topic, sep = "__")))) %>%
  ggplot(aes(term, beta, fill = as.factor(topic))) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  scale_x_discrete(labels = function(x) gsub("__.+$", "", x)) +
  labs(title = "Top 10 terms in each LDA topic",
       x = NULL, y = expression(beta)) +
  facet_wrap(~ topic, ncol = 4, scales = "free")


lda_gamma <- tidy(abstract_lda, matrix = "gamma")

ggplot(lda_gamma, aes(gamma, fill = as.factor(topic))) +
  geom_histogram(show.legend = FALSE, bins = 40) +
  facet_wrap(~ topic, ncol = 4) +
  scale_y_log10() +
  labs(title = "Distribution of probability for each topic",
       y = "Number of documents", x = expression(gamma))

```

I've played around a little with different numbers of topics, and I don't think the gammas are a lot better with fewer than more topics - although, selecting only four topics (for example) makes the topics a lot more intelligible to me. 

# Authors per publication over time
Here's a simple thing we could have done much earlier: does the number of authors per publication change over time?

```{r authors_per_pub, cache = TRUE}
aut_pub <- author_df %>%
  group_by(id) %>% 
  count() %>%
  full_join(author_df[,c("id", "year")], by = "id") 

ggplot(aut_pub, aes(x = year, y = n)) +
  #geom_point(alpha = 0.1) +
  geom_hex() + 
  scale_fill_viridis(option = "A") +
  geom_smooth(method = "lm")

fit <- lm(n ~ year, data = aut_pub)
summary(fit)

```

Well, it looks like number of authors per year is increasing by about 0.18 authors per year - but there's some funky stuff going on with the residuals of that model, so I'd be a little hesitant to trust those findings.

# Frequency of common terms in abstracts over time
Take a look at how the most common terms in abstracts have changed over time. 

```{r terms over time, cache = TRUE}

#Add year to abstract word counts, and count number of times each word occurs in each year.

df <-  full_join(abstract_df, author_df[,c("id", "year")], by = "id") %>%
  group_by(year, word) %>% 
  count() %>% 
  arrange(year, desc(n))

#Figure out most common terms and see how they've changed over time (could also choose terms of interest in another way):
top_words <- df %>% 
  group_by(word) %>%
  summarise(total = sum(n)) %>%
  arrange(desc(total))

top <- top_words[1:10,]

plot_dat <- df %>% filter(word %in% top$word) %>%
  filter(year < 2017) %>%
  filter(year > 1980)

ggplot(plot_dat, aes(x = year, y = n, group = word, color = word)) +
  geom_line() 
  
```

If we're interested in doing more with this, two good next steps would be to normalize the data by total number of words in the corpus for each year (the corpus gets bigger over time, so words naturally become more frequent) and to think more about what words we're actually interested in. It might be that combinations of words are really the most interesting (bark beetle? forest carbon? stream temperature?).

## Semantic Networks
### with keywords
Another thing we can do is look at which keywords or abstract words act as bridging concepts by treating the keywords or terms in abstracts as nodes in a network, where the edges are papers. Let's start with keywords for simplicity. Earlier, we already made a network graph of keywords. Another way of assessing this is by testing the centrality metrics for different keywords. 

```{r setup data, bin keywords}
unique(keyword_df$keywords[grepl("climate", keyword_df$keywords)])
climate_words <- c("climate change", "climate", "global warming", "climate-change")

unique(keyword_df$keywords[grepl("mountain", keyword_df$keywords)])
mountain_words <- c("mountain", "mountains", "mountain areas", "mountainous regions", "mountainous terrain")

keyword_df$keywords[keyword_df$keywords %in% climate_words] <- "climate change"
keyword_df$keywords[keyword_df$keywords %in% mountain_words] <- "mountain"

keyword_pairs <- keyword_df %>%
  pairwise_count(keywords, id, sort = TRUE, upper = FALSE)
```


```{r keyword network metrics, cache = TRUE}
graph_obj <- graph_from_data_frame(keyword_pairs, directed = FALSE)
ec <- eigen_centrality(graph_obj)[[1]]
ec_df <- data.frame(ec) 
ec_df <- ec_df %>% mutate(name = rownames(ec_df)) %>%
  arrange(desc(ec))
names(ec_df[1]) <- "eigen_centrality"

bet <- betweenness(graph_obj) %>%
  data.frame() 
bet <- bet %>% mutate(name = rownames(bet))
names(bet)[1] <- "betweenness"

deg <- degree(graph_obj) %>% 
  data.frame()
deg <- deg %>% mutate(name = rownames(deg))
names(deg)[1] <- "degree_centrality"

keyword_stats <- full_join(ec_df, bet, by = "name") %>%
  full_join(deg, by = "name") %>%
  select(name, everything()) %>%
  arrange(desc(degree_centrality)) 

#Add the frequency of each keyword for reference. 

freq <- keyword_df %>%
  group_by(keywords) %>%
  count() %>%
  rename("name" = "keywords")
keyword_stats <- left_join(keyword_stats, freq, by = "name") %>% arrange(desc(n))

kable(keyword_stats[1:20,])

#Choose the most frequent keywords. 
keyword_stats_top <- keyword_stats[1:100,] %>%
  filter(name != "climate change") %>%
  mutate_at(vars(ec:degree_centrality), scale) #scaled to mean = 0, sd = 1

plot_dat <- keyword_stats_top %>% data.table::melt(id.vars = c("name", "n"))

label_names <- c("ec" = "Eigenvector centrality",
                 "betweenness" = "Betweenness",
                 "degree_centrality" = "Degree centrality")

ggplot(plot_dat, aes(x = n, y = value)) +
  geom_smooth(linetype = 2, size = 0.5, color = "grey20", fill = "grey80") + 
  geom_text(aes(label = name), size = 2, check_overlap = TRUE) +
  facet_wrap(~variable, labeller = as_labeller(label_names)) + 
  theme_few() +
  labs(x = "Number of occurrences",
       y = "Scaled value")

```

The highest keywords in this list should be those that are the best connected to other keywords - that act as bridging concepts. However, they may also just be the most common ones. Another way to think about this is to see which keywords rank the lowest - which are not bridging concepts?

```{r lowest ranked keywords, cache = TRUE}
kable(tail(keyword_stats, 20))
```

Maybe there's a way to control for the frequency of keywords, if that's something we're concerned about. 

### Make keyword networks with and without mountains. 

```{r mountainous}
#Avoid including "mountain pine beetle" as a mountainous keyword. 
for(i in 1:nrow(keyword_pairs)){
  if(grepl("beetle", keyword_pairs$item1[i])){keyword_pairs$item1[i] <- gsub("mountain", "mtn", keyword_pairs$item1)[i]}
  if(grepl("beetle", keyword_pairs$item2[i])){keyword_pairs$item2[i] <- gsub("mountain", "mtn", keyword_pairs$item2)[i]}
}


mountain_network <- keyword_pairs %>% 
  mutate(mountainous = ifelse(grepl("mountain", item1) | grepl("mountain", item2), "yes", "no")) %>%
  filter(mountainous == "yes") %>%
  dplyr::select(-mountainous)

mountain_network %>%
  filter(n > 1) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = "purple") +
  #geom_node_point(size = 0) +
  geom_node_text(aes(label = name), repel = FALSE, #point.padding = unit(0.01, "lines"), 
                 size = 2) +
  theme_void()
```

Which are the top keywords associated with "mountain" keywords?

```{r}
mountain_papers <- keyword_df %>% filter(grepl("mountain", keywords)) %>%
  filter(!grepl("pine beetle", keywords)) %>%
  filter(!grepl("beaver", keywords))

mountain_associates <- keyword_df %>% filter(id %in% mountain_papers$id)
top_mountain_associates <- mountain_associates %>% 
  group_by(keywords) %>%
  count() %>%
  arrange(desc(n))

```

### Make keyword networks with top 100 keywords. 

```{r keyword network with top keywords}
top_kw <- keyword_df %>% 
  group_by(keywords) %>%
  count() %>%
  ungroup() %>%
  arrange(desc(n)) %>%
  slice(1:50)

set.seed(5)
keyword_pairs %>% 
  filter(item1 %in% top_kw$keywords) %>%
  filter(item2 %in% top_kw$keywords) %>% 
  graph_from_data_frame() %>%
  ggraph(layout = "igraph", algorithm = "mds") +
  geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = "salmon") +
  geom_node_point(size = 0.5) +
  geom_node_label(aes(label = name), repel = TRUE, #point.padding = unit(0.01, "lines"), 
                 size = 2) +
  theme_void()
 ```


### semantic networks with abstracts
Do we find the same patterns with abstract words as with keywords?

```{r abstract semantic analysis, cache = TRUE}
graph_obj <- graph_from_data_frame(abstract_word_pairs, directed = FALSE)
ec <- eigen_centrality(graph_obj)[[1]]
ec_df <- data.frame(ec) 
ec_df <- ec_df %>% mutate(name = rownames(ec_df)) %>%
  arrange(desc(ec))
names(ec_df[1]) <- "eigen_centrality"

bet <- betweenness(graph_obj) %>%
  data.frame() 
bet <- bet %>% mutate(name = rownames(bet))
names(bet)[1] <- "betweenness"

deg <- degree(graph_obj) %>% 
  data.frame()
deg <- deg %>% mutate(name = rownames(deg))
names(deg)[1] <- "degree_centrality"

abstract_stats <- full_join(ec_df, bet, by = "name") %>%
  full_join(deg, by = "name") %>%
  select(name, everything()) %>%
  arrange(desc(degree_centrality)) 

#Add the frequency of each abstract word for reference. 

freq <- abstract_df %>%
  group_by(word) %>%
  count() %>%
  rename("name" = "word")
abstract_stats <- left_join(abstract_stats, freq, by = "name")

kable(abstract_stats[1:20,])
```

What are the least well-connected words in abstracts?
```{r tail abstract stats, cache = TRUE}
kable(tail(abstract_stats, 20))
```

Plot a network of abstract words: (this is words that show up together 70 or more times - totally arbitrary)
```{r abstract network, cache = TRUE, fig.width = 10}
set.seed(1234)
abstract_word_pairs %>%
  filter(n >= 70) %>% 
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = "springgreen4") +
  geom_node_point(size = 2) +
  geom_node_text(aes(label = name), repel = TRUE,
                 point.padding = unit(0.2, "lines")) +
  theme_void()

```

## To dos
1. Check whether calculations are using number of connections or not. 
