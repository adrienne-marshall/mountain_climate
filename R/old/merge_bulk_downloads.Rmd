---
title: "Merge bulk downloads and filter for exclusions"
author: "Adrienne Marshall"
date: "4/1/2017"
output: html_document
---

```{r setup, echo = TRUE}
#Set working directory.
home <- "/Volumes/research_storage/mountain_climate"

#Load libraries
library(tidyverse); library(knitr)
```

Get web of science data. For now, only keep the article information and our decision - not all the rounds.
```{r get_wos, cache = TRUE, warning = FALSE, message = FALSE}
#Read recs without comments.
wos <- read.csv("data/wos_recs.csv")
wos[wos==""] <- "NA"

#For this script - merging - keep only identifying information and round 4 answers. 
wos <- wos[,c(1:13, 34)]
wos <- wos[,-8]
names(wos)[13] <- "include"
names(wos)[1] <- "id"

wos <- wos %>% mutate(source = "web_of_science")
```


Now get CabDirect.

```{r get cabdirect, cache = TRUE, warning = FALSE, message = FALSE}
cab1 <- read_csv("data/cabdirect1.csv")
cab2 <- read_csv("data/cabdirect2.csv")
cab <- bind_rows(cab1, cab2)
nrow(cab)
```


Fit cab to approximately same columns as wos.
cab doesn't have disciplines (or wos_id - but it does have an identifier of its own, called PAN)
It does have some other interesting information, but we could look at that later. 

```{r manipulate cab}
cab2 <- cab %>% 
  dplyr::select(Authors, Title, `Document title`, `Languages of Text`, `Item Types`,
                Descriptors ,`Abstract Text`, `Year of Publication`, Doi)
cab <- cbind(cab2, cab[,1])
cab <- cab %>% mutate(id = NA)
cab <- cab %>% dplyr::select(id, everything()) %>% 
  mutate(include = NA) %>%
  mutate(source = "cabdirect") %>%
  add_column(disciplines = rep(NA, nrow(cab)), .after = 10)

names(cab) <- names(wos)
nrow(cab)
```


Filter cab in some obvious ways - first, keep only journal articles in English.

```{r filter cab}
cab <- cab %>% 
  filter(type %in% c("Journal article", 
                             "Journal article; Conference paper",
                             "Conference paper; Journal article")) %>%
  filter(language == "English") 
nrow(cab)
```


Now, let's merge the cabdirect articles with the web of science articles. 
```{r merge cab and wos, cache = TRUE, message = FALSE, warning = FALSE}
wos$wos_id <- as.numeric(wos$wos_id)
dat <- bind_rows(wos, cab)
#How many papers do we have?
nrow(dat)

#How many unique dois do we have?
length(unique(dat$doi))

#How many unique titles? In a perfect world, this would be the same as number of unique dois.
length(unique(dat$title))
```

Wow, those numbers are not very similar. Because there are more unique titles than DOI, I _think_ it's better to keep by doi - which seems less prone to spelling errors anyway. How many new papers does that mean?
```{r number of new papers}
length(unique(dat$doi)) - nrow(wos)
```
Alright, I guess that's not too bad. Let's not actually filter by DOI first - let's first do some exclusions, and see how many we end up excluding from each of these sources. 

Now, use the exclusions function to determine some exclusions. I took river names out of this because there were too many rivers with very short names (e.g. "ob") that were making us lose papers we wanted. 
```{r make exclusions incl abstract, cache = TRUE}
source("exclusions_fn.R")
dat_keep <- exclusions(dat, abstract = TRUE)
#Test to see if our exclusions look like things we want.
excluded <- dat[!dat$title %in% dat_keep$title,]
excluded <- excluded %>% select(title, abstract, source)
#kable(excluded[1:6,])

nrow(dat_keep)
#How many exclusions came from web of science vs cab direct? 
a <- dat_keep %>% filter(source == "web_of_science")
nrow(a)
b <- dat_keep %>% filter(source == "cabdirect")
nrow(b)

```


There are quite a few papers in there that mention areas we're not interested in (e.g. Klamath), as well as areas we do want. Let's not include abstract text in the exclusions, then. 
```{r make exclusions no abstract, cache = TRUE}
dat_keep <- exclusions(dat, abstract = FALSE)
#Test to see if our exclusions look like things we want.
excluded <- dat[!dat$title %in% dat_keep$title,]
excluded <- excluded %>% select(title, abstract, source)
#kable(excluded[1:6,])

nrow(dat_keep)
```

This looks better! Everything getting excluded looks like stuff we don't want. We didn't exclude as many, but that's okay. 

Now, let's filter to only unique DOIs. We'll preferentially keep papers from web of science because they already have a decision associated with them.
```{r filter to unique DOI, cache = TRUE}
dat_distinct <- dat_keep %>% filter(!is.na(doi)) %>%
  distinct(doi, .keep_all = TRUE) %>%
  distinct(title, .keep_all = TRUE)
doi_na <- dat_keep %>% filter(is.na(doi))

dat_new <- bind_rows(dat_distinct, doi_na) %>% 
  arrange(desc(source))

nrow(dat_new)

```

With everything done, how many new papers do we actually have from Cabdirect?
```{r test new}
a <- dat_new %>% filter(source == "cabdirect")
nrow(a)
```

Well, that's sort of a lot, but I guess it's manageable? Might as well assign new reviewers while we're at it. We also need to change a few variable names and assign ids to cabdirect articles. 
```{r tidy}
dat_new <- dat_new %>% rename(database_id = wos_id) 

#Throw out duplicate titles that have NA doi (only 3)
dat_new <- dat_new %>% distinct(title, .keep_all = TRUE) 
  
#Assign ids to cabdirect articles. 
dat_new <- dat_new %>% arrange(id, desc(source))
n <- max(dat_new$id, na.rm = TRUE)
m <- nrow(dat_new[dat_new$source == "cabdirect",])
cab_ids <- c((n+1):(n+m))
dat_new$id[dat_new$source == "cabdirect"] <- cab_ids
```

Now, assign reviewers.
```{r assign reviewers}
names <- c("Adrienne", "Becky", "Danny", "Micah", "Courtney", "Paris", "Shana", "Meghan")
new <- dat_new %>% filter(source == "cabdirect")
n <- ceiling(nrow(new)/length(names))
names <- rep(names, each = n)

dat_new <- dat_new %>% mutate(reviewer = NA)
dat_new$reviewer[dat_new$source == "cabdirect"] <- names[1:nrow(new)]

write_csv(dat_new, "data/wos_cab.csv")


```




